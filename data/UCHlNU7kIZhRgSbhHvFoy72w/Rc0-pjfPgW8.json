{
    "transcript": "# \ud83e\udd17 Hugging Cast S2E6 - Scale LLMs with Intel Gaudi and Xeon #\nhi everyone and welcome to hugging cast hugging cast is a live show about building AI with open models and open source and in each episode is about learning how to build AI with up models using the work that we do with one of our partners through live demos and our goal is that you can walk away from this Having learned practical examples that you can apply to your use cases in your company and we want to continue making this show live and interative active so after the Demos in in about 30 minutes we'll start taking your questions from the live chat you can send them whenever you want and today we're going to talk about our work with Intel and show you how you can use uh it and build AI that is super efficient you're going to learn about the latest features of xon CPUs that are super easy to find and use in cloud and we're going to show you the latest Intel AI accelerator gudi 3 so please give it up to Reis a and elas who will be running the demos Reis is core maintainer of optimum Habana for Gaudi and a and elas are co- maintainers of optimum Intel do you guys want to say hello hi hi everyone awesome well before we get to the demo I want to give you some context we've been working with Intel for many years now about four years uh to enable all the things that we're going to be be showing you today and so I want to show you how you can scale uh gen workloads large language model workloads with Intel gudi and xon CPUs the purpose of our partnership with Intel is to progress open science for the community to provide open source solution for the community and enable AI Builders and talking about our partnership for up science a great example of that is a very recent blog post you can find on the hugging face blog about Universal assisted generation and how you can get faster decoding with any assistant models to tell you how open science turns into open source that turns into new experiences for AI Builders a great example I like to use is set fit set fit is at the beginning a paper a paper with joint research from hugging face researchers with Intel labs researchers and upk upk labs to propose efficient F shot Learning Without prompts from the set fit paper came the set fit open source library that makes that implements the paper and make it easy for the community to implement set fit meth methods uh to do more efficient training of EMB bidding models and from this open source come solutions for AI Builders and a great solution is a recent blog post that was published uh by the Intel team along with the hugging face team to enable blazing fast set fit inference accelerated with Optimum Intel on xon so in the summary of this blog post we were able to increase throughput by eight times for the embeddings model so it's super impactful for the community for all the applications that you guys are building today using embedding models and I mentioned Optimum Intel what is Optimum in Intel Optimum Intel is an OP Source Library you can find it on our GitHub that provides a bridge between all the great uh open source libraries to accelerate workloads from Intel AI libraries like ipex libraries like up Vino libraries like Intel neural compressor and the hugging face libraries that you use daytoday like Transformers diffusers accelerate and many more so with Optimum Intel we will show you how you can accelerate your workloads uh for uh for Intel CPUs and you can find it uh very easily we have a great documentation on our website uh so you can learn how to quantize to accelerate to deploy accelerated models using Optimum Intel okay and a partnership our partnership for AI Builders so from all these great open source Solutions we want to provide you with easy to use examples to apply this in your day-to-day work so a great example of that is a recent blog post that shows you how you can accelerate star coder with Optimum Intel leveraging Advanced quantization Techniques and all the latest data types that are available in the latest generations of Intel xon CPU including speculative decoding so check out this blog post in this blog post we're able to accelerate through all the different steps of quantization all the way to assisted generation to a 7x speed up uh for uh for the the large language model workload so that means that uh you can really consider uh CPUs as a Workhorse for your machine learning workloads a lot of people in our community really think about gpus first and you should really consider how you can apply all the advanced uh techniques and data types and quantization techniques that are available for CPUs to really scale your workflows and apply um apply as many training and inference workflows to this more efficient more scalable uh sort of platform So today we're going to show you two platforms and and solutions working on two platforms I talked about Intel xon CPUs they're readily available in all the different clouds and we're going to talk about gudy AI acceleration ERS uh that are available in many uh many places so the latest uh for Intel xon CPUs something that you're going to learn you're going to hear over and over again starting with the fourth generation that is code named safire Rapids uh Intel introduced Advanced metrix multiplication specially designed for machine learning workloads that really accelerates M matrix multiplication uh and uh gen workloads and along with AMX came support native support for new data types like B floats uh bflo 16 uh integer 8 uh to accelerate through quantization your machine learning workloads and all this you can use very easily using Optimum Intel with gudy AI accelerators the latest and greatest is the third generation of gudy accelerators U that's offers 128 gbit of ram to um to store ever greater bigger models and to have even faster throughputs um gd3 is two times acceleration on in fp8 versus the previous generation four times in B float 16 and you can enable you can use all that very easily using our library Optimum Habana and a great way to put it all together um is through this sample solution offering an endtoend rag uh retrieval augmented generation application that is fully powered by our Intel stack so you have an llm that is being run on gudi here on G3 available through Intel Dev Cloud you have an embeddings model that is run through Tex embeddings inference our embeddings production solution running on a sixth generation xon CPU um everything hosted within Intel Dev Cloud Al together in a complete solution that did explain in this recent blog post called build building coste efficient Enterprise rag applications with Intel gud 2 and Intel xon all right there's just so many resources so many examples available on higging face for you to see how you can leverage Intel platforms to do more efficient more scalable uh machine learning now I've talked enough I want to hand it over to uh to Reis to show you how you can take advantage of Intel gudi 3 uh today for your machine learning workloads thanks Jeff thanks for the the great introduction so yeah as you mentioned you talked about G3 so it's quite an exciting time to to work on on this accelerator because it has been like publicly available for just for just a few weeks now uh I put the link here uh like if you want to access it go to the Intel AI Tyber Cloud uh it's uh it's pretty easy to to launch an instance and to get started in a in a few minutes so don't hesitate to to take a look at it um and just yet to give like a bit more information details before the the demo um so basically uh it all it first started with Optimum Habana which is like the first uh umide the first page of our partnership with Intel and that the library you will use for example if you want to use Transformers diffusers or some of our other libraries um so it's will still be very easy to use like for example if you have a Transformer script working on GPU you want to Port it to gudi and to run it uh there will just be a couple of very small changes and like the main models are optimized for for GDI but this is done like and other food so you don't have to uh really care about that and uh the other part of uh what we have been doing with with Intel and gudi is TGI and TI so Jeff Jeff talked about it uh so this will enable you to serve to serve your favorite llms or your favorite embedding models in an optimized way for gudi and that's perfect for for your uh production use cases um and um so basically I'm going to now uh show you a demo of of TGI on on G3 um so uh you can go to this uh first link uh basically to to to take a look but I'm going to to show you actually uh a bit more how how it looks like and then after that in just two commands I'm going to show you you can uh get started like to La the server and then start sending request to the server and get the the output so I'm going to do a demo with Lama 3.1 7B instruct so this is a checkpoint you can uh find on the Hing face Hub so I'm going to share my screen now to show you how it looks like so yes I'm sharing my screen um so here I am on the uh homepage of the tji repo on GitHub uh so basically so there's a lot of code here uh but what's interesting here is is the rmy uh there is a table at the beginning if you want to check like all the models that are currently supported so we have llms but we also uh have some multimodel models uh so we have lava for example uh so there is like support for bf16 for fp8 also since sc3 is compatible with fp8 uh and then either single card of or multi card not all the models can uh hold into the memory of of a single device and basically just to get started uh so as I told you there are just two commands to to to know uh so we have already a doc image that is ready to to pull so you can just copy it paste it and then in a terminal so here I'm connected through SSH to gory instance so this is a gory instance from the Intel AI tber Cloud uh so just to show you a bit and re do you mind zooming in a few notches uh yes yes yeah there we go awesome so basically here uh so Jeff talked a bit uh about it but basically this is a g dist server you have like eight devices and you have like about 128 gabyt of memory on each device which uh is quite a lot and which will enable you uh to really fit very big models big llms uh into uh into memory and so as I told you so we have the doer pool command so you can just do this here the do image is already in cash so it's not going to download it because otherwise it would take a bit uh longer so for for the sake of the demo it's faster that way but um that's a very simple common to run and after that uh basically so the SE command so you will have a doer run command so here this is like the first section of the rmy just to get started but after that we have like some more specific more optimized commands for several models for in bf16 precision and in fp8 Precision so the one I want to show you today is uh Lama 317b on eight cards uh to make the most of the G distri server we we have um so here so you have some uh environment variables uh so you can just quickly export them so here this is the name of our model on the Hing face Hub uh the HF token so I have a token which is already registered uh then the volume so I already have maybe zoom in uh sorry maybe zoom in this to yeah there we go thank you so yes so here we have we can specify a volume I mean this is basically a directory where like tji is going to download the weights of your model uh so that if you laun a server several times it doesn't download them every time like just basically just going to download the weights once and after that uh if you laun the server again with the same model uh it will it will find the weights were in this uh in this report um and uh so uh basically so there is this command that basically you can just copy paste um so this is the command you want to run uh to laun the server uh so so this is the docent command with like many different arguments you can take a look at the documentation of TGI and at this FR me if you want to understand a bit more what each of these argument does but that's basically yeah the command you want to to run so uh here so as I told you the model uh the weights of the model were already downloaded so uh basically it doesn't download them again and then what it's uh going to do is to um so the model is shed actually and we have eight devices so we want uh to uh send one chart on uh on each device uh so this this is what it's currently doing so it will uh take a bit of time not too long but this is basically about copying uh from the CPU the host Ram to uh copying the model from the CPU Ram to the uh device memory um okay that's that was not what I expected um why there is device memory here device memory still live demo effect yeah no that's on one count yeah that's this one I wanted to show on G3 that was not yeah that's on eight count so that should be this run okay so let's start over again that should uh that should work um and uh so basically after um associating all the shards I mean the sharts to uh every device uh basically you will see there is a warm-up phase where uh basically what you have to know is that the first pass through the model the first forward pass generation pass is always going to be slower and since we don't want users to be affected by it when you launch a server there is a warm-up phase um that uh basically is going to take place right after this you will see there will specified in the and this form up phase takes usually like one minute one minute something because we perform some forward path of the models for several input sizes uh to make sure that uh afterwards users are not affected by it and they can get output uh quickly um so yeah so that's um that's what happens when uh you you laun the server so here's the command I used U basically so this is uh for pf16 precision if I'm not mistaken yes it's it that's it uh there are also commands if you want uh for fp8 uh I didn't do it here because basically what you have to do for fp8 is you usually uh you have to go through first step of uh measuring some statistics on uh like some um prototype data and we have a script for this this will generate uh a Json file that then can be used and specified through this environment variable and after that uh your model will be able to run in fp8 but the measuring part the first step uh can take uh some time so for for the demo it was a bit uh too long but yeah that's it so here you can see that the model is warming up so that's what I was uh talking about so right now it's just try a few sizes like a few inputs with different sizes doing a few forward passes of the model so that uh then users just can get their results uh very fast and there will be no no warm up um and after that basically the server will be uh ready to send request to um so yeah so that's it right now we are good to go uh so the server is uh is ready um so if you want to send request uh here is an example of request in the r me that uh you can send you can take a look at the documentation of TGI to know what how you can modify this request to specify a few more things I'm going to show you a few things you can also also do a few cool things but of course uh if you want to know more you can just uh look at the at the documentation so here I just copied uh this so I have a second tab here where I'm also connected through SSH to the same gist server because here in this demo I'm running the server uh in the same place where I'm sending the the request uh so I past the request and uh basically uh here for example generate stream so it's uh basically streaming the outputs so we can see that uh I have my final generat take the output which is uh here uh and um basically uh I know that was the output yeah it was talking about the rabbit I find like yes that was the output for for lava yeah I was like yeah this is a bit we uh I mean it works too but uh we usually like yeah let's let's rather use this one because here the input was what is this a picture uh so basically just yeah I mean the model just say okay this is a picture of of rabbit but let's try with this one for example so this one is not a streaming so you see the difference we just want to generate some text uh so the input is just like okay we ask a question to the model and the model returns uh like this output which uh makes uh which makes sense uh so if we try it again this is gitty search by default so we get the same uh the same output uh so you can can try it as many times as you want with G search the result is deterministic so you you will always get uh the same result if you want for example to change that and perform sampling uh you can specify very quickly this argument the do sample argument and set it to true and we should get uh a different types of different output so here you see we have a different output uh if we run it again we will get another output because here it's doing sampling on the uh most like most likely tokens to to to be in the output so basically each time you run it you will get another another output and then so we saw it with actually the first request uh with the the rabbit request which was not actually the the request I wanted to show you at the beginning but you can stream the outputs which means that instead of just waiting for the server to compute the output and just get the output you can just here specifies the generate stream uh request and you would just uh get like streaming of the of the output so you can see for example here token eight the index of the token and the associated uh text to this token and you can see this for all the generated tokens and at the end uh you will get uh the the complete the complete output so of course there are other parameters that you can uh change uh so for example here if you want to generate like instead of 32 let's say 132 uh it's going to generate 132 tokens uh and you will get a longer output um and then as I told you uh you can configure the generation process the way you want there are many parameters that you can modify so you can take a look at the tji documentation for for that and uh yes that's basically everything I wanted to show you fox um so as I told you there is also an fp8 version for for the models so you can run it with TGI um and uh and the multimodel models also so lava is also available so you can send an input with some text with an image and the model is going to uh like return an output uh which will be able to like for example caption uh the the image or like if you want to ask the model what what's in there what's in the image the model is going for example to to be able to to do that so yeah that's everything I I wanted to use so thanks yeah I'm going to that's really really cool so um to uh to recap uh gudi are Intel AI accelerators they're alternatives to gpus that are custom design for AI workloads to accelerate AI workloads for training for inference gd3 is the latest generation we make it easy through our library Optimum Habana and our production inference solution TGI gudi uh and you can find G3 AT Intel def Cloud you can find it on Denver Cloud you can find it on IBM cloud and of course uh you can contact the Intel team to get your own uh gd3s um next I'd love to hand it over to elas and a to show you how you can accelerate uh your AI for Intel xon uh CPUs uh so yeah take it away yes so uh just as Jeff said uh we also have Optimum Intel to accelerate your U gen gen generative Ai and machine learning workloads on CPU and also on GPU and xpu for that we integrated two uh very fast run times very accelerated run times which are uh in extensions for py and open Vino uh these two Frameworks come with some advantages and disadvantages each so uh as you might have noticed uh in intal extensions for pyour ipex for short uh you will get something that's very close to Native and Native pyour and with open Vino you will get something closer to an experience that's closer to Onyx runtime and Onyx so uh one works in an eager mode and the other will work in graph mode and uh yeah for with these two Frameworks we try we try to make them like as closely and as tightly uh integrated with the huging face ecosystem so if you are used to using if you used to Transformers in diffusers uh Auto models and the auto pipelines uh you will notice with the our integration Optimum int that uh the integration is almost the same and the usage is almost the same you will just have to change uh some class names and that's all you don't need to change a lot in the your workflow and so for example uh we present how to use open Vino runtime for open Vino you can either uh do an export beforehand if you want to always like have an exported version of your model in its graph mode uh you can do that using the optimum CI command uh it's very simple it's a single line uh command that uh will store your exported model and uh will allow you to reuse it uh uh later later on uh with the library uh you can also uh perform uh exports on the fly with the OV model and the OV pipeline uh classes these classes come to replace uh for example Auto model for coal LM or Auto pipeline for texture image uh both are classes that are very used from U from Transformers in diffusers and uh you can drop in replace them uh with the their all they open V Alternatives uh and you will uh get speed ups and uh accelerated uh inference uh just like that out of the box uh we support all kinds of tasks with the with these p with these open Veno models and open V pipelines uh ranging from uh text processing classification uh text generation and also uh uh lately we also integrated VMS so we also support like complex uh pipelines like U like vlm LS uh where we accelerate each component encoder in the language model uh we also support all three main uh tasks in the diffuser space so we support text to image uh image to image and image in painting uh we late uh we lately added support for the biggest models that the biggest release models like flux and stable diffusion 3 uh and the yeah with all this uh there's also a second component that you can use to accelerate U uh your inference which is quantization and I will let uh Ella uh explain to you some key Concepts about that and how to use it to come back later uh with a demo uh that about uh what kind of inference speed up uh that you will get uh on on CPUs awesome um so now to talk a bit about quantization um so um we when you use Optimum Intel um you have the possibility after the open exports that were just mentioned before by elas you have the possibility to also quantize your model um so uh the principle um of quantization is to use a lower BWI um to represent your weights and your activations um the advantages is that um for example if you have a very big model that can't fit in memory it can reduce the memory needed to store this model it can also um make your INF faster and um reduce also the energy consumption to run your model so it's overall something um very interesting because it's usually very easy to um to apply and um can you can benefit from it a lot um so something very simple that we um always um advise to to first um do is to apply quantization on your weights um this um can be very interesting because it usually results in a very low um degradation in accuracy um so yeah very very easy to to implement especially when you do e a quantization um you can very easily use the minmax methodology um which means that you will take the mean and the max of your tensor to compute your quantization parameters like your scaling factor and zero points for lower bit tws like four bits we also provide a methodology which are gptq andq both of those methodologies require to provide a calibration step but um they allow to apply quantization in it4 which is um yeah usually much more difficult because it impacts uh the model accuracy much more um so the disadvantage of applying quantisation only on your weights is that the activation would stay in the original precision and that during inference you will need to cast your weights in the same precision as your activation which means that this will introduce some overhead and also because um because of this it means that um your you will not benefit from an yeah integer arithmetic operation so now something that you can do is that in addition to quantitizing your weight you can also apply in quantization on your activation uh for this you have two possibilities um so either you um do Dynamic quantization which means that the ranges of your tensor will be computed at run time so during your inference um this this um add some overhead of course because you need to yeah compute those ranges during inference um but Dynamic quantization can be very interesting because um it can very much more adapt to um input of varying distribution so this can be interesting um and we also provide um static quantization in which um the quantization parameters are fixed so to do this you need to provide a calibration that is again and perform a calibration step um to estimate those ranges um so it's very interesting because you don't have to compute anything at from time you already have all your um quantisation parameters so it's very interesting but it can impact a bit more accuracy so what we usually advise to do is to first start by weight on quantization after that to check if um activation can be quantized dynamically and if this works well um uh the third step would be to do a calibration step to to check whether this is um this is possible really cool actually it's a good point to ask a question that was asked uh by Jess vender in the chat um which is so you you pres you showed us like all these different ways of quantizing a a model and you talked about the impact on accuracy and J vender was asking like for a typical 7 billion parameter uh model like what should people expect in terms of accuracy hit uh when you go from 16 bits to or 32 bit uh to 8 bit uh data types um so it's it's very difficult to answer to this question because it varies of course from um on what exactly um what types of quantization you applied so with which methodology whether you did only on the weights usually quantizing the weights only um works very well so let's say you have an 8B uh model it means that you it will fit if it's in eight in with 8 gigb so it's can be very interesting and usually result in not too much accurac degradation this um can be a bit more complicated when you quantize the activation as well so usually it's even more complicated I mean you can expect a bigger um degradation in accuracy when you have a decoding phase because this will result in an accumulation of of error because you continue to yeah use um activations which have been quantized so we've lost a bit information um so it's it's very difficult to to say that in general this um this uh this you can expect that type of of uh um of um degradation in accuracy especially because uh in it also depends on how the model was obtained So currently we are talking about post trining quantization so depending on how the model was trained you can have a a big impact like for example whether you've used um yeah bf16 or fp16 to to train the model this could impact um the how easy it will be after that to apply a post quantization um usually bf16 um results I mean if the model was trained in bf16 it's easier to to apply quantization yeah also yeah yeah but but this being said like we have some examples right and we have some examples uh with blog post that report exactly like what uh where the accuracy uh impact at every step of the quantization I had shown earlier uh blog posts uh from the Intel team along with us uh showing star coder 15 billion um in how it was quantized using smooth Quant and I was surprised that the reported metric I think human eval actually went up uh by half a point about uh from the Baseline in 16 when uh it was quantized using smooth quants to8 bit weights and activation yeah yeah it's very interesting um so when you apply quantization something that that can happen is actually to have an even better um accur accuracy results because it can make your model a bit more robust actually um and uh yeah it adds some um yeah regularization so it's can it can happen that it actually improve the over accuracy but I wouldn't say that it's a yeah so for the for the business person like me it's like yeah actually you can get as good model uh the difference there is always an impact but like it can be very very very small yes yes awesome sorry sorry to uh to interrupt no no it's a it's it's a good point to to add yeah so now we will discuss a bit about different demos that we've done um so now that we've discussed about the different step um that we did so first um the export step afterwards quantization and after we also discussed about a bit how to use your model and to um yeah perform inference with it um so we will discuss a bit about how we can do all the different steps um so we've created um an export step which uses Optimum Intel uh to easily um yeah convert your model to the open Vino intermediate representation format um so first you need to make sure that you're correctly logged in after that you can use um yeah a model that's um on the Hub as long as it's supported by Optimum Intel but for now we support a large amount of different architectures um and um yeah you can very easily like this conver a model uh by default um we make the public the uh resulting repository public but you can uh set it to private um if if you want to and once um the conversion step is finished what will happen is that um a new repository will be be created under your namespace um with the resulting open V model so for example we can see it here um so yeah the resulting model um you can see how to load it with Optimum Intel and how to yeah perform inference after that so once the model is loaded and we can see that we have the yeah the open V model um so now we will also um present another space that we did to to um apply weight on quantization um so you can either provide um a model which was already uh converted to open vino or you can provide also a model which was um which was not um so for this uh for this case we will um use the one which was already converted to not do another step because we we already did um and so uh yeah you can for example here apply 8 bit quantization on your weights um we also provide the possibility to do four bit quantization when you do forbid quantization we also um leave the possibility to provide a calibration data set um to do the awq um quantization that we presented earlier and finally for forbit quantization we also provide um a ratio parameter which defines um the number the amount of weight which would be quantized to forbit um so in the case where we have 0.8 it means that 80% of the weights will be quantied to for bits and 20 will be quantied to 8 Bits in order to reduce a bit um the impact it could have on on accuracy um so here um we have the resulting model which was quantized to 8 Bits And um we can see um when we compare it to the original model so this is the fp32 model which is um three um 100 megabytes and the 8bit model resulting um yeah we can see is four times smaller so uh it's what we would expect when we go from fp32 to in8 um and now that we've shown spaces to export your model and to also apply weight on quantization um we will also show um a space um that is um showing um inference using latent consistency models um so the model um that we are using um was quantized um with open Vino n ncf so with Optimum with the that we just uh showed and um yeah very easily um so you can yeah have a prompt and in less than than 5 seconds or around five seconds you have um the generated images that you can check wow really fast yeah it's h it's pretty nice so yeah so this is um I think it for the different demos showing the different step that we can do to with open Vino um and now I will uh let elas talk a bit more about the benchmarking uh aspect uh of it uh yeah so um since we talked a lot about performance and accelerating uh the workloads uh we also have a library for benchmarking uh the performance of different uh back ends uh especially our uh optimal set packages and the the Frameworks they bring into the game so uh in Optimum Benchmark which I'm gonna share my screen yeah uh so yeah Optimum Benchmark is basically our unified multi backing utility for benchmarking all kinds of workloads uh all kinds of tasks on Transformers or diffusers and uh it's a very simple and easy to use for uh Library so you just install the library and uh it's after that it's a single line Optimum Benchmark you specify a configuration directory and a configuration name for the configuration name you will need uh something like this so I'm going to show you an open V example so let's say for example this you specify the kind of scenario for example here we want to do inference uh we want to see see how like uh the speed of our inference workload the back end for example open Vino a launcher uh for example here we use process and that's to isolate the Benchmark um and yeah and the the other things are mostly uh how you want to customize your benchmark so uh here we also do some quantization on our weights uh we use a birch model and we also do something uh with is statically compiled the model so open Vino allows you to uh to use it with uh Dynamic shapes for example inputs with all kinds of shapes but you can also reshape it which means uh make those Dimensions more static and uh that will also like bring you some speed UPS so uh we Benchmark memory latency everything we support everything Optimum Benchmark I really advise you to use it if ever you want to compare uh multiple Frameworks for example pyour versus open Vino versus ipex and know which one is the most performant for your use case I also built a space to simplify this process so uh I deployed the space on um on a CPU an xon CPU that supports uh all kinds of optimizations we find in open Vino uh like uh the AMX instruction sets everything and of course the data types uh and with this space you can actually run the benchmarks uh on this machine so for example here I run a benchmark with the uh just simple gpt2 on the task text text generation and I enabled some optimization so I enabled the statory shaping so so that I can accelerate my open Veno model and I also made sure it's compiled after it's reshaped and I also applied uh weight only quantization using the load in four bit uh argument and with this uh I was able to get for example if we see the throughput of the prefill stage uh so prefill is when the model uh an a caal model like gpt2 or Lama is first uh launched it will have to process the whole prompt uh so uh we show you here like around 2x more than 2x uh speed up uh in the prefill stage and more than two again more it's around 2.5x in the decode stage and this is the most important stage in a CM model it's actually the decode because you'll want uh your user to have like a very fluid experience uh uh with tokens that arrive uh in a very fast and streaming way uh we also uh have a measure of uh the per token latency so this is what happens at decode we receive one token at a time and you will see that in the per token latency it's much much faster it's actually 7x so if we look at to p50 uh latency which means that uh 50% of the tokens that we measured uh is actually faster than uh 10 milliseconds on the py side uh we have around 7 79 milliseconds so that's that's a very high speed up of course the speed up is not on all tokens because uh the CPU also will um well this is something very common in benchmarking we will have like uh some steps that will be slower than others that's why we run a lot of steps for example here we actually run around [Music] 2,500 uh tokens and 50% of them were at around uh 13 milliseconds so yeah uh I think you will enjoy your using this uh this space it's uh open for anyone to use uh it is limited though because there's a Quee of size one so uh we can't run benchmarks in parallel uh or like concurrently to be as uh uh to be as true as possible in the uh in the metrics and the results we we achieve so that's why we limit this to only run one Benchmark at a time uh after you run the Benchmark so for example I can run it yeah another thing that I forgot to to specify that this framework also supports numactl this is something uh you'll have to deal with if you have like a a very big CPU the ones you find servers with multiple nodes uh and the many like CPU cores uh so yeah we also like can specify things like how many tokens we want to generate and for the fusing for the fusers uh how many inference steps to run Etc so it's very simple to use you just run the Benchmark The Benchmark will be saved uh on a in a folder on your uh account so for example for me it was saved here in ilas muchi benchmarks and I can come here and find my true benchmarks uh the performance and configuration of open open Vino and pyour for pyour I have everything here with the results I just showed you so yeah and it's very detailed like you have everything you have like uh the p50 as we said it's uh 79 milliseconds in pyour and open Vino we can find the report can go down here p50 t13 so yeah and uh you have a very like holistic and at the same time fine grained uh view of the performances you will get so uh I really advise you to use this uh whenever yeah this is Benchmark for py is done and now it's running for open so yeah I advise you to use it whenever you are confused between the different Frameworks which one to use just run the Benchmark and get the the metrix um The Matrix will never lie to you so yeah thank you so much elas thank you Reis thank you a we're a little bit of a time uh we've done our best to try to answer questions as they were coming uh through the chat um a very typical question thanks fim is like will the recording of this be available yes it will be available you can come back to the same link to find it and we will publish it on the huging face YouTube account um another question that was asked is [Music] um here Ed Ed was asking um is there also support for assisted generation uh speculative decoding um and the answer is yes actually one of the posts that I've cited is specifically about this with the example of star quarter 15 billion running on the fourth gen Intel xon CPU so Sapphire Rapids uh where we are able to accelerate it seven times using assisted generation and speculative decoding check out the the blog post and I think that's um that's about it uh thank you so much to recap we've showed you uh how uh you can accelerate your gen llm deployments on gd3 the latest generation of Intel AI accelerators um and you can use it easily using Optimum Habana and TGI gudi uh We've showed you how you can compress reduce the size to fit larger models to have higher throughput uh on Intel xon CPUs leveraging all the latest uh AI techniques available like Advanced M matrix multiplication all the quantization techniques and even pruning as a um mentioned available in Optimum Intel so that you can use the scalable uh xon CPUs the available xon CPUs on every cloud to power your gen use cases and finally we showed you how you can run your own benchmarks with Optimum Benchmark to easily see as you go and quantize using different techniques what is the impact for your model uh on all the different metrics that you care about thank you so much team thank you Reis thank you IIA thank you a and uh again the recording will be available soon and with this bye",
    "video_metadata": {
        "title": "\ud83e\udd17 Hugging Cast S2E6 - Scale LLMs with Intel Gaudi and Xeon",
        "published_at": "2024-12-12",
        "view_count": "782",
        "like_count": "29",
        "comment_count": "1"
    },
    "comments": [
        {
            "comment_id": "Ugzp8ddhABcVtnFx9u54AaABAg",
            "video_id": "Rc0-pjfPgW8",
            "author": "@joeyspizz",
            "date": "2025-02-10",
            "likes": 0,
            "comment": "Is this a podcast. Can we find it on Spotify and Apple?",
            "num_replies": 0
        }
    ]
}