{
    "transcript": "# \ud83e\udd17 Hugging Cast S2E5 - Introducing HUGS - Scale your AI with Open Models #\nwell welcome again uh to this very special edition of uh hugging cast it's a very special edition because today we're going to be announcing a brand new inference solution called hugs and here's how the next 40 minutes are going to go down first I'm going to tell you all about it uh then philli is going to show you how it works next we have a very special surprise guest coming coming on to the show I'm really excited to bring on and last we'll have some time for questions and answers please keep asking them in the chat as we go all right so today we launch hugs hugs stands for hugging face gen AI Services it's a new commercial solution to help companies put up models to work you know hugging face offers so much for free there are over a million models today that are free and accessible on the Hub there are all all of our open-source libraries and so I get the question a lot how do you guys make money well today I'm excited because I can say that you know models are free but we charge for hugs what is hugs hugs is an easy way for you to deploy the most popular open models in your own secure environment and get the maximum performance es without having to do any configuration and why is this important why do we need hugs you're probably familiar with this chart from Maxim Labon in the community that shows you the rate of improvement of open models catching up with the best closed models available and that pace of improvement has been accelerating over the past year and and a half so today on the hugging face Hub there are several options available with up models that offer similar capabilities as the best closed models out there and here we're just talking about performance on academic benchmarks what matters for your use case is different and with fine tuning with reinforcement learning you can improve upon these open models and make them so much better than closed models and it's just not only a matter of accuracy open models be closed models also because you can use much smaller more specialized models to control your cost as your usage grows and there are models that you can host yourself in your own environment so that you can keep your data safe not send it over the internet and have a much better posture with regards to compliance because you know exactly what you are using so why do most companies I talk to today tell me that they've built their initial proof of concept concept their initial AI applications on top of closed model apis well I think one of the reasons is because going to production with an open model today is harder than just sending yes um I I think we cannot correctly see the slides we still see the plan of attack now we see it okay gotcha all right uh then I will share my uh screen differently share my entire screen and go back here do you see uh do you see what I'm saying yes now we are seeing going to prod is harder than heading in API okay so when I said you're Pro probably familiar with this chart from Maxim Labon you probably were wondering like what is he talking about uh well what I was talking about is this chart I'm sure you've seen it uh on your social feeds shows you the rate of improvement and uh so yes with OP models you have a better security posture compliance posture and you can control your costs better and the reason why today most companies have started building AI applications with closed models is because it's very challenging to go to production when you start dealing with gpus how do I prevent these out of memory errors if I want to fit within this GPU do I need to quantize in which data type using what algorithm then if I want to accelerate by compiling the model you need to think about the back end that you're going to be using and then as you deploy the model you have to figure out the right set of configuration variables so it works so it's a lot of uh trial and error and usually that means that for an experienced machine learning engineer is going to be at least a week of hard work and that's why why uh we built hugs we designed hugs to give you an outof thee boox solution that just works when you want to deploy for production an open model so how does it work uh so at the heart of the hugs service is the hug model deployment container and the hug container is optimized end to end from the hardware to the serving software to the configuration and the model itself to give you great performance out of the box everything is built on top of hugging face open source libraries and the container can be deployed easily within your own environment maybe it's your uh Cloud uh tency maybe it's on Prem if you're an Enterprise Hub customer and so I talk about models and I talk about hardware and at launch today uh we are enabling 13 of the most popular op open models through hugs everything is built specifically for the model that you're going to be using we have some of the most popular upen large language models today from meta from Google with gatu from Mistral from quen and also some amazing fine tunes from the news research group um available and soon we will make available as well great embedding and ranking models so you can build full rack Solutions on top of hugs and I talked about Hardware one of the key benefits of hugs is that it's optimized end to end to exactly the hardware that you put behind it and we are doing this for a variety of Nvidia gpus and also for AMD Instinct gpus so whether you're using an h100 or an a100 or an instinct mi3 the hug container will optimize itself and fetch the right assets and configurations to give you the best performance and through our collaborations with AWS and Google we'll be able to soon provide optimized backends as well targeting inferentia 2 ntpu accelerators with hugs all right so that's how hugs work now I want to tell you what's in it for you like how does that help you and this in Five Points so the first point I already talked about a bit which is that because it's an open model and a container that you can run within your own environment that means that you can keep your data safe and you can keep your model inhouse second it's a zero config solution so hug the hug container will optimize itself for the exact Hardware that you put behind it that means that reduce your time to Market your time to take a model to production from over a week to less than an hour third hugs provides you with optimized deploy mment so out of the box you get maximal efficiency and that's true whether you're using any kind of state-of-the-art Nvidia GPU that's also true for AMD GPU and soon other AI accelerators four uh hugs gives you a plug and play solution to transition your existing AI application that may be built on top of open AI or another llm provider onto open llms that you host yourself the hug container once running gives you an open AI compatible API so you don't need to do any code changes to uh Point your app to your open model and last hugging face um open source Technologies are at the heart of hugs and within the hugs container we're optimizing everything uh for you that means that you can think of hugs as sort of an Enterprise distribution of hugs open source technology te Oles that we support that the hugging face team tests and maintains for every configuration for every model and Hardware um uh combination possible and we've given access uh to hugs in early uh stages to some of our Enterprise Hub customers and here we have jlam puta who's an AI builder at Orange and here's what he had say we tried hugs to deploy Gemma to on gcp using an L4 GPU we didn't have to fiddle with libraries versions and parameters it just worked out of the box hugs gives us confidence we can scale our internal usage of open models and here is poly con another AI Builder and Enterprise Hub customer and here's what HRI CTO had to say hugs is a huge timesaver to deploy locally ready work models with good performances before hugsy would take us a week now we can be done in less than an hour for customers with Sovereign AI requirements it's a game changer thank you H thank you jlam all right so that's all great but where can you get hugs and how much does it cost so hugs is available today on AWS and when you subscribe to the listing on the AWS Marketplace you uh the price is $1 per container hour that's running the container is running and using resources within your own AWS tency so the compute and usage costs are built directly by the cloud provider we're working hard to make hugs available on Google Cloud as well through its Marketplace all right so that is hugs really excited about what we're launching today and we already have some great update updates cued up uh for hug customers with more model support uh with improved performances with support for new accelerators like inferentia 2 on AWS tpus on Google and support for fine tuned models so now I feel like I've already talked too much I'm super excited hand it over to Phillip to show you exactly how it works great before over to you yeah before we go into the demo we got some questions um sweet can you maybe explain in a quick uh how does it differ to like Nvidia NS we have like two people asking what's different yeah well it is uh uh I would say completely different in the sense that it is built on top of hugging face open source Technologies um one key benefit of hugs uh is that it's sort of Hardware agnostic right so it will use the right serving stack the right uh data type the right optimizations whichever Hardware you want to put under the container and then we got some two similar questions so this can be deployed on a GPU crit right like how how would I approach this like do I need to come to us to deploy it or how would it work sorry say again I me that one no problem I mean I can answer it so like um we can like go into the demo in a second where we show how you deploy it on AWS and currently so we are planning uh to answer the other question as well to add Hax containers to all public Cloud marketplaces so Google is currently in the making we sadly didn't make the deadline asra is also um coming in the next few weeks so it should be available on AWS on Google on erer hopefully by the end of November and for on premise customers you can directly contact us and then you get basically an Enterprise contract and access to the containers and then you can deploy those containers on any infrastructure you want this can be your own premise cluster maybe you host in your own server Center but it could also still be like a different cloud provider like Oracle or something else so we have like those two different distribution paths with like subscribing through the cloud marketplaces super easy I mean we'll see it in a second it's like one click off a button and it takes like 10 minutes for us to to spawn a container and the other PA is like the traditional Enterprise uh kind of approach where you reach out to us uh we set up an agreement um provide you access to the containers and then you can pull them and work with them in any environment you want okay and yes Max uh the the $1 per container per hour power is besides the ec2 pricing cost which is built directly by your cloud provider cool cool so let's get into the demo so uh in addition to to hugs we we launched a few assets for you to take a look afterwards or maybe to go alongside as well so we have a new documentation for haug it's available on huggingface doco dohu where you can find all of the important things you need to know so we have like the supported models where you have a complete um model to Hardware uh Matrix where you see all of the current supported models here we will add all of the other models Jeff mentioned over time and also where it works so for example if I already know okay I want to deploy llama 70b um I know I want A10 GPU is not enough uh we have all of the Nvidia flavors but also the AMD flavors which are currently supported and if you want to know exactly which Hardware is supported you can find um this in the documentation as well and then in addition to all of like the technical details we have have the different guides on how to run hug so if I want to run on premise I got access through like a huging phas Enterprise subscription we have instructions how to run it uh with simply Docker with Docker compost we have a a cuet section so for hacks we build and maintain Helm charts to make it super easy for you to deploy on any kubernetes distribution available out there but now for us most importantly we want to take a look on how can we deploy it from the Ada WS Marketplace um if you are interested something uh we also includeed so we have the $1 per container hour but the Hing phase offer on AWS has a free trial of five days so if you sorry want to give it a try you have like a f day free trial where you are not paying anything so you can subscribe try it for one two three days just make to make sure that if you don't want to use it shut it down otherwise uh cost will will Ure um in in there we can go to our ha offer you can also find it if you go directly to the AWS Marketplace and then we can subscribe to it I already subscribed oh I think you are not seeing the other window okay there we go so I'm on the AWS Marketplace I have the Hax offer and I already have access to the product uh if you're not having access this orange button here should show something with like subscribe and and when I click on it I again get the terms of service I then go to configure continue to configure and in here we can see the current version and also for which Hardware it is available so in here you will soon see uh AWS in fenia uh and then I click on continue to launch and I have all of the informations available for me to to get started user instructions will link us to the uh AWS documentation with the helm chart and in here you have instructions on how to create uh your IM credentials and also how to access the different containers and the ha offer gives you access to all of the containers so you only need to subscribe once to the ha offer and then you get access to all of the different containers which are included here but also in the documentation so let's quickly jump into the console stop sharing share [Music] screen okay perfect there I see it can you zoom in a bit yeah perfect yes get a bit bigger so I'm basically on an AWS ec2 instance and I already created a kubernetes cluster um the instructions for how to create a cluster are part of the documentation we took a look earlier just to save us some time we already created the cluster we deployed a model that we can test it but we will quickly walk through on how easy it is to to create a cluster so we have created the kubernetes cluster with uh Nvidia A10 gpus and um and load balance available and for us to deploy the a Hax container model we will use our ha Helm chart and normally y perfect so you here you can see all of the values I need to set so I Define a a deployment name that's basically what the homech will use and also all of the how all of the kubernets assets will be named I have a service account that's very important that's basically the name you define when creating your cluster that you can access the marketplace containers and then the node group is okay on which kubernetes node group do I want to deploy my model here the ha node group is the one with the gpus and then I can simply run Helm install uh here the image registry is the image um from the marketplace so when you when you can remember the Subscribe offer on the marketplace had the container U at the last step we have the huging Fest model and we want to deploy CR 2.57 be instruct we have to version 0.1 and then I only just need to run or hit enter and um Helm will prompt the install and what's very nice about is we included steps on like what to do next so okay it's now running okay how do I get access right and as part of the H charts we also create a load balancer meaning we are making the ha container immediately externally accessible you can disable this um via hel chart value so if you say okay I don't want to make it externally accessible I want to maybe uh make it accessible from like a different VPC or just inside my cluster I can also um do that and the easiest path to get access to your incest route so like to the external domain I copy the the slip it from it and when the load balance is ready you will get your url here which you can then copy into your next steps and send a request to it since we I mean we can quickly check into our cluster so we have the Hax quen container that's the one we created earlier which is running but then we also have the Hax demo container which we just created and if we check the locks we can see okay we correctly registered the usage for the marketplace we are on an Nvidia A10 we F1 and we also have the memory and then we have all of the configuration which is used by the ha container and now what happens behind the scenes is we load the correct artifacts for our configuration and then the container is started on the API is made available for us since we already have our container running we can simply copy uh Cur command so we use here our um hug s container which we deployed earlier and Jeff mentioned it um the container is compatible with the open AP OPI API so like basically what every model serving uh solution out there currently uses so we have our endpoints under chats and completion we have our messages and we can ask okay how can we um or what is deep learning and we get back our response and what's super nice that it since it's compatible we can also basically use the real open AI uh SDK so we prepared a snippet from open AI we override the base URL to our kubernetes URL uh and then we have our instruction we have streaming on and then we can run the request and we should see our response being generated and that's that's it for our model deployment and at the end as mentioned just to make sure if you are drawing it out want to save cost make sure to delete it again super simple hel uninstall and then our uh container will be terminated again and we can see it it's shutting down correctly and we are not having any unnecessary cost awesome thank you so much Phillip this is really really cool we really hope that with hugs will make it easy for companies to transition um their their applications their prototypes pointing to model providers apis like open AI uh to build them on top of open models that they can own and host and and control um and at the beginning I promised that uh we had uh one more thing so let me share my entire screen again and go back back uh to those slides so one more thing um can you see the slides with the pricing again yes cool so it's working this time so as I said uh the pricing for hugs is a dollar per container hour and today you can go and subscribe on the AWS Marketplace and start using hugs today but that's not not the only uh way that you can get hugs today in fact you can get hugs without paying a dollar per container hour to hugging face or without paying hugging face anything and I'm really happy today to announce a new strategic collaboration and partnership with digital Ocean and the first thing that we want to do through this uh collaboration is to make hugs available to digital ocean customers for free that is you only have to pay for the usage of the GPU droplets where hugs run this is a new service called oneclick models that's available within GPU droplets on digital ocean and to tell you all about it and to show you how it works I'm super excited to bring onto the stage sanan Nasir at digital ocean who built this amazing integration hi San hey guys I'm really excited to show off this new hugs integration between digital ocean and hugging face uh it's really exciting how this can come all come together with just one click and we'll walk through the whole process and see how can we can create a oneclick model using hugs on digital lotion GPU droplets so uh why why deploy oneclick models on a GPU droplet so digital lotion oneclick models eliminate all infrastructure complexities allowing you to focus solely on building your model in points be it your store chat b or if you want to create a tool to facilitate collaboration across your team uh do is the best platform to do it uh in order to do that uh we'll get started with the create button on the digital ocean Cloud console let me bring bring your screen to the stage there you go yeah okay yeah we'll go we'll go ahead and click create on the digital ocean Cloud console uh and select GPU droplets once we're on the GPU droplet page we'll select the appropriate region uh I'll select Rono because that's closer to me uh then I'm going to scroll down to select an image uh we already have AIML images but the new uh addition to our catalog is going to be oneclick models to reveal all of the new models uh we'll click on this oneclick models Tab and with just one another click we can select any of these models from the list for this demo uh I'll go with the meta Lama uh variant and to select its 8 billion instruct version so I'll go ahead and select that one and I'll make sure that I have a single Nvidia h100 GPU ready and selected uh finally I'll select my SSH key for authentication and then go ahead and create it and that's how fast it is and it should take a couple of minutes for a single GPU machine uh it takes around around 5 to 10 minutes for the 8ex GPU machine because it's a longer pool of gpus uh while this is creating I'll talk more about what powers these oneclick models so the foundation model itself uh runs uh via Docker containers uh using images that are provided by hugging face these have uh inference service built into it so you can just deploy the container itself and start inferencing uh these are managed through Docker compost uh through our cust Docker compost files that support these GPU configurations uh we also have caddy to uh expose the inferencing apis to the outside world this is all secured by access based token authentication so you can feel free to access this API without any without any risks of your data getting leaked uh to speed things up I'll have another identical machine that I created in advance that we can go ahead and start inference I'll go ahead and select this GPU to view its details I'll go ahead and copy the droplet IP and to view the access token I'll have to open the web console while this is connecting we'll have to wait a couple of seconds and yeah so you'll see the token in the message of the day uh we go ahead and copy the token itself and we're good to go now that we have the public IP and the token itself we're ready to make some API calls so I already have a cur command prepared with the relevant IP and the token already plugged in so I have this playground environment that I'm going to use to test the service so one thing I want you to note here is uh although it shows that the droplet is active uh the inferencing service takes around 5 to 10 minutes to warm up the model and be to become available after creating the GPU droplet so uh again this GPU has been created in advance so we don't have to wait that long but as a future uh disclaimer to creating new droplets uh it should take a couple of minutes but it's faster uh so yeah let's get started to create uh some Cur requests I'll copy the the Cur command here and just run it and you can see the request was sent successfully and the inferencing service returned a successful response and yeah that's it now really really fast yeah uh and you're you're free to customize uh these requests uh according to your use case uh these parameters are configurable according to the open API spec provided by hugging face and yeah that's how quick and easy it is to deploy oneclick models on digital ocean droplets via hugs awesome really really cool thank you so much San for showing this uh it's so cool to see hugs already in action and so to recap on digital ocean you have this GPU droplets uh service that makes it super easy for you to use Nvidia uh h100 gpus under the hood and within GPU drop there's this new oneclick models option where you can find all the up models ready to deploy everything powered by hugging phase hugs thank you so much uh for this great uh great demo um I'm really excited to um to hear some questions from from the audience I know a bunch of questions have popped up in the in the chat already so let's uh let's take some of those yeah we have like immediately a question for San like what is the minimum rent duration for droplets if you have an infrequent service can you just pay for the duration of the inference or do I have to rent by hour or day so you just need to pay for the droplet itself so droplets are charged by the hour so right now we have a offer going on for single GPU droplets for 339 and yeah you can just create a droplet for an hour and test however many models you want and then droplets are using Nvidia h100s right yes that's that's correct awesome really cool um I see a a few questions uh about uh fine-tuned models like can I bring my own fine-tuned models can I bring my own Laura uh adapter to a hugs experience so yes and no uh currently you cannot bring your own Laura adopters but that's basically what we are working on so all of the models you have seen supported and the more models will be we will be add can be used as a base model for your Lowa adapters and then you can provide them through a hiding phase repository it's currently being built should be available hopefully soon so all of the models you see we support out of the box can be used as the base model for fine tuning and then if your lower adapter on the High face hop available you can use them to basically override or replace the the base model uh weight awesome thank you yes this is dais zero for for hugs and a related question but different question was from Christopher was asking is there road mapap for other models like stability 3.5 flux or other models and as I said earlier uh everything within the H container is really optimized for one specific model so that we can really test it and make sure that it works out of the box with the best performance possible uh so that's why it is specific to each model so we do have a road map of adding new models with hugs optimization so you can benefit from this Enterprise distribution of hugging face open source with uh these open uh models as I said uh one of the first things that we want to do uh is to enable uh embedding models uh and ranking models so that's uh companies can build full rag Solutions full semantic search solutions to ly on top of hugs uh but uh yeah of course happy to get input from the community on like what would be the most useful uh to them I don't know Philip if you have other thoughts no yeah the best best thing you can do is like similar to what you did share with us like what you are the most interested in what we or the road map basically is always based on like the trending models on highing phase so if you leave a like to your model it's already the the right step and then we'll continue continually to uh add more and more models especially over time first day first release but we plan to extend it to a lot of more and different models as well awesome thanks we also had a question from Amit who is asking does hug support serverless option to avoid costs during idle hours and here you know the the the the whole point of hugs is to enable companies to bring their AI production in in house to host their own models and so in that sense uh we're really not designing it to be a a serverless solution but a solution where you can host your own models on your own infrastructure if you need like serverless options to like test and prototype things we have other things at hugging phase like the inference API for which you can get better rate limits by subscribing to Enterprise Hub Etc hugs is really designed for production that you want to internal and run inhouse cool I think we got another question for sonan from Raymond is there a tool to estimate the total cost how many users per or like tokens per second can a single droplet handle so the total cost will be the cost of the GPU droplet however uh we're still since hugs is new we're still benchmarking and calculating the total uh requests per second and the round trip time it takes for a single request so yeah stay tuned for more info on the details page but we do have the max tokens and base info about the models already on the catalog awesome cool so tonic is going to dust off um their digital ocean account that's nice um and then Jean Luke was asking if you don't support serverless then do you support scale to zero instead so so hacks is really not a managed solution I would say so there is no API you would hit directly it's really we created this artifact and you can do whatever you want with it like we make it super easy for you to deploy on the AWS Marketplace uh AWS going through the marketplace Google Aer digital Ocean or on premise if you on ENT Price customer we might use or use already ha container inside the inference API to test and prototype but it's not really what the ha product is is meant to be so if you have an internal kubernetes cluster with for example K native or like a similar container serving tool yes then of course the container itself can scale to zero but it's depending on where you put it into the solution and what we should always keep in mind um those llms are very big so if you scale them to zero it will take time to load the model again start the model and the bigger the model gets the the more um time you will spend on it so I would probably rather look at the use cases where you know I have workloads I have a running model I have users and if not maybe look more at like the API kind of products um before like moving into a real container solution thanks Phillip um question from Andreas are you using text generation in inference uh to within hugs or something else yes so we will you or we are using and we will use text generation inference and I'm not sure if most of you know but TG or text generation inference is not Nvidia GPU specific we have a great collaboration with AMD to enable TG on AMD we are working with Nvidia to enable TGI Plus trt llm on Nvidia gpus we have with Optimum neuron and AWS a collaboration to make it available on uh inferential devices with Google on TTU so the whole TTI um service or solution is really this now multi backend solution and we have like different backend serving engines for like the different Hardwares so Nvidia we have like our TG engine and also soon trt llm for AMD we have the AMD engine uh on AWS we have Optimum nurun and for Google we will have chat stream and Optimum TPU awesome and uh one more from Denis uh will this be available on dedicated inference on hiking face and I think by that you mean where you're using hugging face compute as opposed to your own compute um so in a way we already have a solution for this it's called hugging face inference endpoints and it's a solution that's really model uh agnostic multicloud multi kind of Hardware where you can select any kind of model and select the cloud you want to deploy it on and and select the region and the instance that you want to deploy it on and will all set itself up uh automatically um so we already have sort of this solution uh for uh for customers who want to run their models on hugging facee compute um hugs is really designed for companies to internalize and host themselves their own models with the best performance available um does it make sense to uh offer hugs uh through Enterprise Hub on inference endpoints I don't know it's an open question maybe at some point in the future all right well I think that was our last question thank you everyone for tuning in uh thank you San for joining and showing us oneclick models on digital ocean this um uh video will be available on demand if you come back to the same URL we will then publish it on YouTube thank you again for tuning in and you can go today uh try hugs out on AWS with a fived day free trial is that is that right yes and on digital ocean awesome thank you so much Phillip thank you San bye everyone thank you so much",
    "video_metadata": {
        "title": "\ud83e\udd17 Hugging Cast S2E5 - Introducing HUGS - Scale your AI with Open Models",
        "published_at": "2024-10-24",
        "view_count": "934",
        "like_count": "34",
        "comment_count": "4"
    },
    "comments": [
        {
            "comment_id": "UgwY3WSzOsnUl1Bek_Z4AaABAg",
            "video_id": "m6CGGPbwHCY",
            "author": "@jmirodg7094",
            "date": "2024-10-24",
            "likes": 1,
            "comment": "So you're not giving free hugs anymore\ud83e\udd2a. Haha at the end you still gives a few free hugs...  this looks a lot like google's VertexAI on GCP but with open models and optimization",
            "num_replies": 0
        },
        {
            "comment_id": "Ugy_CmbbfmwjOnHayVd4AaABAg",
            "video_id": "m6CGGPbwHCY",
            "author": "@TheAIPivot",
            "date": "2024-10-24",
            "likes": 0,
            "comment": "I'm so excited for this! \ud83d\ude31",
            "num_replies": 0
        },
        {
            "comment_id": "UgzhvIwJDH_E_9wQX014AaABAg",
            "video_id": "m6CGGPbwHCY",
            "author": "@brianhopson2072",
            "date": "2024-10-24",
            "likes": 0,
            "comment": "Is this part of Ollama's integration with HF?",
            "num_replies": 1
        }
    ]
}