{
    "transcript": "# Hugging Face Journal Club - DeepSeek R1 #\nall right let's get into it you guys can see this right y um I'll give my take and then we can discuss some of the details so what's really cool about this paper is that it's very simple and quite short um so in a nutshell they explored roughly two two directions in in using reinforcement learning for llms the first one is what they call this uh deep seek r10 model where they just use GPO um on the Deep seek B3 model and found that if you just have verifiable outputs you can actually get really good performance but then they found that the model although it was good at like math and code um kind of sucked at like General language tasks and was kind of unusable or they say you know suffered with readability and so then what they did in instead was they used uh that model uh to generate um a large amount of or actually sorry I think they collected a large amount of sft data and then using the sft data they then did the kind of conventional rlf Pipeline with the twist that you know you're Now using verifiable outputs and this gives you the Deep seek R1 and someone I think on Twitter posted this kind of like nice image uh sort of showing the the different uh like pipeline so you can see here for example on the on the left hand side like if we just look at this if I draw a picture here so if we go kind of like down here this is the kind of r10 so it's just pure RL on the deeps V3 base but then in order to kind of improve its like you know human preferences and all that kind of stuff they then took a different approach where they have some unspecified amount of like cold start Chain of Thought data they do sft and then they basically then go through this like kind of Fairly complex process of like creating an sft model applying rejection sampling to it so on so forth and then at some point at the very end now applying kind of RL and that gives you the DSE i1 model um so yeah the paper is cool it's uh very simple um and it feels like this is probably quite close to what you know 01 or open did because they kind of were hinting that they didn't use any search uh algorithms explicitly and stuff like that so this is perhaps the first case we see of you know a pure RL kind of no heuristics based method working um the other thing they did which was quite cool was they then you know took outputs from the the Deep seek R1 model and then they just generated uh a bunch of um completions and then just sfed uh a bunch of these like smaller models and they show later in the paper that basically this kind of distillation um actually works better than applying like their RL recipe um which again perhaps is reflective of what we've seen before where people who were kind of distilling from like gbd4 and just doing sft on that were getting really great results um without the need of any like you know fullon rhf um so that's about it um so let me just go through to the bits that I thought were kind of interesting so the thing that I thought was like obviously very cool is that they just use this like simple prompt here um basically to take the the base model so the Deep SE V3 base model and then just using that prompt they're able to um basically RL from this um without any sft data so that's that's pretty cool the question I had and maybe I don't know if you guys noticed this or not um is this the like just the base language model of V3 or is it like the instructor model the impression I got um from just briefly reading the paper is this is this is the base or at least they start with the Bas uh and then at some point they use this version of the model to generate sft data to do an instruct version which they then tun with RL but this like all the Impressions it's just the base let's just see do they say here um without even using it supervised wherever you just were they mentioned it yeah yeah yeah yeah so so I understand that they don't use any um like supervised data um I'm just trying to see where is the initialization so the reason I asked this is like if we go to oops go deep speak V3 there's kind of two models right so there's V3 base this is the base model um but then you've got deep seek V3 which is then the the actual like you know um kind of instruct model and so the reason I'm asking this question is like if they started from this guy and then did all the IRL stuff um that's one thing it the fact they do it without any like sft data is one thing but it would be less impressive than if they really did it from from the base um but um this there's a paragraph I think above where they talk about this uh I saw it earlier yeah even the title on the basement yeah the overview part [Music] there you're saying like up here it's like uh in 2.1 overview in this study we demonstrate the reason capabilities can be significant Pro blah blah blah even without using sft as a cold start yep okay that's very cool so so what I guess what's impressive right is that the the base model is essentially able to kind of follow this uh this like prompt right um it reminds me a little bit of like you know anthropics old paper on like um using like LM as an assistant sorry using like assistance as a Laboratory um for reasoning and stuff where they had that like very long prompt of like you know 4,000 tokens to kind of induce a chat into the base model um but here it's impressive that just with that small amount of um uh of instructions it's able to then you know generate these tags I think as well because at the first stage they focus on reasoning tasks which are verifiable they DOL just like stuff where they have the ground truth and it's just zero one like rewards that maybe makes it more simple like more stable to train than you've got a reward which you can hack um yeah yeah they do mention that actually here where they say we do not apply outcome or process reward models as we find it can from Ro hacking um yes so that I thought was pretty cool um and of course you know this plot is like pretty awesome showing how you go um over time the essentially reaching 01 level performance um then there's like this um this whole kind of um well actually this is also a very interesting plot right it's showing that effectively like the thoughts I just getting longer and longer uh over time um so it's kind of like uh you know it's like test time compute during training um which is like quite interesting be interesting to know if can you hear me yeah y um it be interesting to see if um it's stabilizes at some point because I guess that in the data set you don't need like millions of reasoning steps for most of the prompts maybe at some point it stabilizes in the value that like the mean reasoning steps that you need to solve all the the questions in the data set yep yeah like what's interesting also is is kind of that um we also know like in other things like with a DPO producing long answers is not necessarily you know good like you know non- reasoning based benchmarks and so here it's kind of like interesting that like if I ask the model like you know what is one plus one and it takes like 8,000 tokens to get an answer that's obviously not super helpful which I think they say later where they do this like kind of more sophisticated multi-stage training to kind of you know blend like reasoning traces with like non- reasoning ones um then the other thing that's very cool is this like aha moment where they sort of realize well they see that in one of the thoughts um the model is kind of you know taking a different um strategy um that's like pretty cool I think also what's like for me very interesting is like the uh appearance of these like uh hm weight kind of tokens because I've always wondered like with1 like how did they induce that was it like something that was done through like prompting or through like annotation and at least here it seems to just be something that emerges uh naturally from the data or sorry from the RL process and presumably because the base model has like somewhere in its way seen this kind of like stuff in in the pre-training it could be the the the word model as well because you have the thinking tokens and once it sees those in the in the format you mean this kind of stuff yes and so it's kind of rewarding it for having this thinking stuff and [Music] then yeah so so then let's look at this how they train the actual one model right so they talk about okay um they they need like some sort of cold start and so they say they need some small amount of long cot data um and then they did a whole bunch of stuff so they basically used F short prompting um and then Gathering also outputs from the RL model and so on and then they say that they got you know some thousands of cold start data so this is now I think going through the kind of like um uh standard RL process where you kind of sft uh you know your your base model and then based on that you now um can do the rest of the process um so the exciting thing here right obviously is that now everyone has access to deep seek r10 uh you can just do this yourself uh for any kind of you know domain task you care about which is pretty cool um they take like um a few approaches um do they mention like do do they have results of what approach gives what results I don't think so we'll have a look in a sec but they seem to have just used like a bunch of different things right so just fuse shot prompting uh probably an existing probably the Deep seek V3 model um and then doing this like kind of reflection verification and then using the RL stuff I bet there's actually a fairly large amount of worked down here by human annotators um to get this to be high quality so it's it's not like a free lunch um let's have a quick look actually at the eals to see if you're kind of asking the question of like uh did they ablate like the sft versus the the RL no I guess if you upate theft you just have this d three a dips um model without loal starts which is not readable right yeah yeah I guess what we don't know yet from just this is like what is your starting performance right like from from this once you do this like um uh sft thing so you finetune V3 on the cold start what what kind of performance are you starting with because you know this this base model is already very good so um um yeah then they have this thing where it's now okay you've got this like sft model um and now they're going to do basically RL on just reasoning um and then they have this like language consistency reward to prevent this like language mixing stuff which is kind of interesting um in fact you know Ed you remember when we were doing um the using the Quin math models this is very similar right they they used to do this kind of like language mixing uh and it's probably related to like the the training process yeah and it was more prevalent in their RL versions I remember the smallest model I don't think is try you didn't really see it in the seven higher it was it was obvious yeah when you say language mixing is it like at some point it switches to Chinese or to I don't know Chinese yeah yeah what's interesting is also the the Llama model does that if you uh um like the 1B llama model does it if the temperature is too high so it might be something peculiar about these kind of like models that are probably trained with a bit of AR um and so then they kind of add some penalty in the in the reward module for for such if they detect it right so to stop from doing but I think what they they just probably count right like if you look at the The Chain of Thought So what's in the thinking block you probably just count like how many tokens belong to the the language and then you measure it to probably the the the prompt right so if the prompt is in English you expect a certain fraction of English words in in the thought and then you just have the penalty on that um actually speaking of like the rad models let's maybe look at that quickly before we move on so they had this um two simple rewards right so you've got accuracy which is just checking if the response is correct that's pretty easy um and then you've got these format rewards so in addition to having an accuracy you want to format reward which kind of encourages the model to put its thinking process between think and think um would that just be something simple like a regex like you just check like okay um here's like when I'm doing RL right I I gener my completions and then I check um you know in that output how many like you know which ones basically produce the the correct format and if they don't I get a zero and if I do I get a one is it something as simple as that could be or inside the think if it you know it might even give higher reward if it finds words like wait oh wait or aha or what if or I don't know but yeah yeah I think they're really just specific they're talking about this like formatting right like I think how do you get how do you get the model to basically follow this right so the reasoning process and answer are enclosed within think and think and then answer and answer tags and so I guess like what I'm trying to understand is like this this format this function right it probably just takes the output checks that like that there exist think and think tags checks as content and if not you just get Negative reward and if you do then it's positive right makes sense and they might ensure that between answer and answer there's actually just the answer with very little kind of you know there shouldn't be any kind of working in the between the answer Tes for example but how would you check that with a language model yeah I mean it should be concise and contain like just the latch with the math answer for example it shouldn't contain like 10,000 characters or or whatever yeah yeah so that's pretty interesting and then okay so that's the for zero right that's right but I think they use the same approach when they do like the the R1 right so it's like okay after we F tune on the cold start we now do the same large scale RL process we employed for R zero so now I guess using the same like reward functions plus this additional language consistency thing um there must be another RM for the more ambiguous uh prompts like for the Lang English language prompts or like uh you know the ones where the answer aren't verifiable yeah but I think though they they do like um uh I think it's like this multistage thing right so right now I think we're here so you've sftd the um what's it called on the cold start data yeah and and then they've done RL on just the pure reasoning like examples right so so at this stage in the um in the training you're just doing reasoning and then they do this like additional St stage of training where they say okay when the reasoning R RL converges we now collect new data for the subsequent round and so now they collect data from other domains um and then they basically yeah they they collect like this like kind of 600k data for reasoning um from the model um and then they talk about that they collect non- reasoning data um basically by you know generating from deep cv3 and stuff like this yeah I was referring to 2.3.4 actually where they do the second round of RL there they have to another reward modela yeah yeah so just to check so here we find tune the base for two EPO using the data set okay so they've gone back to the base model at this point so basically once you create yeah basically you're back to here now it's like you've got that you've got your reasoning data and now you're doing this like kind of sft the two EPO yeah so you're right for General data we resort to reward models blah blah blah so so I guess what's interesting here right this is very similar to like the um the Tulu three approach right where they did like you know kind of Po or I think it was even DPO on like you know human preferences and then they've got this additional step where they did like the RL on the verifiable rewards sorry verifiable outputs um but yeah like super simple pipeline conceptually I would guess totally insane engineering um because you know these models are like 600 billion parameters um like they must have the most incredible code base ever to do fast generation checkpointing RL all that stuff very very impressive um and then the other thing is when they just do distillation right where they basically now take the final R1 model they then um have these 800k samples uh and then they just distill on top um and what's interesting is like that in the evals look down here actually here in the evils you can see that like just distilling on quen 32b gives like way way higher performance than trying to apply the RL pipeline um directly and so this I think is like kind of you know again confirming this thing that we talked about earlier of like you know when like um kind of chbt came out many people were like oh you have to do RL you have to do rlf but we saw you know many like examples in the community that if you just sft on like high quality gp4 data you get a very good model and I think this is kind of again confirming that that point that if the model is like small enough distilling from a very very capable model will give you you know good enough performance um yeah and the dis skinning was uh just uh generating data from the big model and then doing sftd on the smaller mod yep they yeah they say up here um when they do the distillation so we directly fine-tuned models on 800k samples created with deep C car1 on your diagram with all the steps y uh where is it so there it seems to imply that it's the same data they used for the sft stage uh right so you know this this combined sft data 800k samples like on the bottom right so this diagram I don't know if it's correct it seems to imply this the same sft data is used for the distillation as is used to train the R1 model before then the RL is applied again so it's not generated from D R1 this this from yeah I think you're correct you're talking about this block here right like is this sft data being used in distillation and as the essentially the init for like RL um and I think you're right because if look here um you can see here right it's like okay um when the RL part converges we use it to collect sft data so they do that whole thing right and I guess at this point right the the model that you have it's not really r10 right it's kind of like r10 with a cold start and now you use that to generate sft data and then say you know we fine-tune deeps V3 for two EPO on that data set and then they say we Implement a secondary RL stage so we train the model blah blah blah and then we use this so I think essentially what's going on here right is like you basically have kind of like a a bootstrapping process it's kind of like at the start you've got your base model so you've got like your base model here right um is my picture you got your base model here and you do this kind of like um cold start or you know pure RL process that gives you kind of now something that you can use to um essentially generate synthetic data then you generate this like you know kind of like reasoning data from itself or also from you know uh other sources and you combine it and then you again now start again doing sft down here and then finally RL on top so you're right Ed it's not the the kind of outputs that are distilled don't seem to be coming from the final R1 model they seem to be coming from these other sources which is a bit surprising by the way we probably better data if you use T car one yeah yeah I'm pretty sure uh I'm pretty sure someone like noose is gonna uh distill the hell out of it which would be pretty cool it um is it maybe the diagram which is false because the name is quite misfeeding like deep R1 distill blah blah blah well that's what they call it right they call like I mean they they they call the model I think R1 distill right so um yeah we have open source dist well okay they say six models distilled from R1 right um the other possibility right is that they do this process to train R1 then they reuse the same 800k prompts to generate a new set of like samples right that would be one alternative um but they don't seem to be saying that because they say okay we directly fine-tuned using 800k samples curated with deep seek R1 as detailed in this section and here they're doing all this kind of stuff yeah maybe maybe it's just a language thing maybe what they really did is what I said that they they trained the R1 model then they probably just reuse these prompts and then do this like rejection sampling plus you know other stuff here um because otherwise effectively what it is is its distillation from deeps V3 and the intermediate checkpoint that they got uh from you know the cold start right so I think the diagram matches like what's described in the paper but it's not actually clear if it's more like this right that it's the R1 model doing that um yeah I I think it's uh it's awesome um very few details right on like the the whole training stuff but um you know now we have GPO in Terell I think there's like a lot of cool things um to try oh one one thing I wanted to ask uh so there's this metric called cons at 64 which I think is what consistency at 64 my understanding of this is that they are using this as an estimator of pass at one so you know generally if you do like pass at one if you just generate one sample with some like sampling you have like you know very large fluctuations especially on like am24 which only has like 30 problems um so the alternative is that you generate like you know many samples and then you use an estimator to say okay given that number of samples what is like the true or like you know potentially true passit one value um was that the same interpretation you guys had or something different I didn't know Chon normally it's like magic whatever so um yeah but Ed remember like when we did the um test on compute thing right we we had um a method for estimating the um uh the kind of like passet K scores right and what we did was we generated like 2,48 samples and then used that equation from the Codex paper to basically estimate the kind of true paet K and I suspect this is similar um but it wasn't like very well described I think they say here yeah we generate 64 responses per query to estimate pass at one so what they they estimate they they do it 64 times and take the average at the end of the day uh for pass at one yeah I think that's yeah I think pass at one is that correct 64 independent pass at ones sampled and they take the average of well it's not that you do 64 independent pass at once it's more like this um uh where is it here right so uh Instead This is like from codex right you generate n greater than K samples and then you use this estimator um to get the the kind of true pass K so oh yeah remember yeah yeah okay yeah so I guess what they did is they just computed 64 um and then it's yeah it's going to be basically 64 choose one so yeah probably just the average um but yeah so there's the section on on what didn't work that's also quite interesting Ying the PRM yeah they they they tried P yeah I wonder if this is like um possibly like a reflection of the fact that like they're dealing with such a a powerful model right like so you know they've got this like half a trillion parameter model and so kind of like just from a pure computation perspective doing like a PRM based approach is not only probably just hard engineering wise but maybe it's just very hard to get like the high like data that is high quality enough to train like a very good PR like a capable enough PRM yeah yeah I think this is probably like you know this is probably the biggest point is like once you have a reward model you just have to keep retraining it which is like a pain in the butt okay good to know yeah and I think the MCTS one is also interesting right like that um they they didn't get to work and they they said they say this thing here which is like okay to facilitate this we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search and so you know because at the moment right the model is doing this like thinking stuff and so in like you know conventional like MCTS if you what you would do is you would like generate at like some point you know maybe like a new line or you know double new line and then you would generate multiple times and estimate like the kind of you know value of a node by doing rollouts to then work out you know are you correct or not and here they seem to be saying that they generate multiple tags that correspond to specific reasoning steps so they must have had some additional formatting to kind of indicate you know up until which point you you roll out yeah maybe as part of the prompt maybe you have a sub sync T tag or token yeah yeah I mean they also talked about this thing right that the the value model influences the quality of the generation since it guides each St of the search process and you know training this like find Grand value model is difficult blah blah blah I think one kind of question here right is a little bit like if you did like the kind of prime RL method um maybe that would actually work but again like if you've already got something that is already working on a simpler pipeline why add complexity but you think that it would work like is it related to MCTS or yeah so in the prime RL well not the hair conditioner um where is the notion yeah just on the top yeah yeah so here they do this like clever thing where effect effectively the the value model is being updated um during the training let me see yeah algorithm one I yeah exactly so but here right they have this like implicit PRM which is kind of getting updated on the roll outs and they they they explain a bit later on how it makes a big difference um here exactly so you can see here right like if you just have some fixed PRM you you still get better but like not as well as like if you have a kind of online PRM right um and I think here in the um uh deep SE paper you know they they talk about like okay the value model influences the generation and tring of fine grain is inherently difficult which makes it hard so I suspect that you know maybe if you combined MCTS with this like Prime RL thing which is more online it could still work but again it's probably like added complexity for potentially marginal gain yeah yeah okay I get in that um but yeah there's like I think lots of very interesting questions right so first of all like there there's no kind of like replication of this open version of this so I think figuring that out is like pretty exciting um and also there's like kind of like you know bits and pieces of data sets available like on the Hub but there's like nothing that's like kind of coherent and what's kind of unclear actually to me is when they did the r uh r10 stuff so they just say that they basically trained for like many thousands of steps but I don't think we really know yeah so you can see here right they trained for like 8,000 steps but the question is like is this like you know a million samples or is it a 100 million samples it's a bit unclear to me unless you guys found out from the paper yeah they don't give much detail of hyper parameters so yeah have idea how big a bch is so yeah and this this might actually be like the kind of key missing ingredient right is like you know maybe it's just not enough to have like a million samples you you maybe you really need like you know 20 million samples of like high quality verified data right right so so here it is the uh Improvement that you get with the RL stage yeah exactly so this is 8,000 steps of pure RL and the question I have is like how big is that RL Corpus right because you know they kind of take they they give us some information that okay the sft data is relatively small but the question is is this like uh you know well here this RL stuff here is that like humongous um kind of interesting question um but yeah anything else you guys want to say yeah the the generation part must be quite um engineer like they generate up to 30,000 cens 64 per prompts yeah and they do it like thousand of times yeah I mean I think this uh kimy paper is like also worth looking at right because this came out the same day and it's kind of on the same topic and they actually provide details of the like architecture like um yeah you can see they've got this kind of like um like kind of ARL infrastructure of like you know roll out workers and trainer workers and stuff and then you can see they actually use like a full-on hybrid thing so you've got this kind of like VM like side car and then you've got like Megatron um doing all the the training and so there's probably like a fair amount of engineering um to get this kind of like stuff working together yeah with this etcd which is like a a consisteny consistency server making sure that you know uh these processes are are up something I something that's used in you know uh High availability databases yep yep so it is very much engineering it's crazy the steps they have to go to to get just the weights from Megatron the LM right okay they first have to convert it to HF format then cop sh memory and then update the way you know it's quite it's quite a f yeah I mean I I think you know honestly like doing this stuff I mean maybe they had their own reasons but like the like Nemo align thing is if you're going to use Megatron you probably should just use this right because they have like um what is it it's using like Triton trt I think where is it yeah I forget where it is it's Nvidia right so it's yeah it's gonna be Tri yeah yeah yeah exactly so they send all their stuff with py Triton so I think like yeah once you in Megatron land you should probably just use that um all right so nice have you tried the train model of R1 yeah yeah yeah I've been playing with it um I've only been playing with a small model um and I played a little bit with the 32b model on huging face chat it's um quite good but then let's um let's figure out how to make this work in real life thank you see you guys see you later",
    "video_metadata": {
        "title": "Hugging Face Journal Club - DeepSeek R1",
        "published_at": "2025-01-22",
        "view_count": "48940",
        "like_count": "1673",
        "comment_count": "49"
    },
    "comments": [
        {
            "comment_id": "Ugy-V26K0EhaLWs61yR4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@StEvUgnIn",
            "date": "2025-02-10",
            "likes": 0,
            "comment": "I think there is a proper difference between SFT and distillation. The paper clearly describes R1-distill to be distilled from R1 (671B total parameters MoE model).",
            "num_replies": 0
        },
        {
            "comment_id": "UgwVT9MjFcZneDsHr254AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@StEvUgnIn",
            "date": "2025-02-10",
            "likes": 0,
            "comment": "Something doesn\u2019t add up. Is R1-Zero just based on the DeepSeek-V3 32B model? Why not SFT the largest one?",
            "num_replies": 1
        },
        {
            "comment_id": "UgxYFTtOuD7JBE99PNJ4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@shubhammittal7832",
            "date": "2025-02-05",
            "likes": 1,
            "comment": "Please share the twitter thread where you got the image about deepseek model flow shown around 1:30",
            "num_replies": 0
        },
        {
            "comment_id": "UgxM5FwnTBFIE97HqCZ4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@tee_iam78",
            "date": "2025-01-31",
            "likes": 0,
            "comment": "Thank you for the video. It is very informative. I love this style of content, listening to what experts thinks about it. Is it possible to have a link for the flow diagram of Deepseek-v3 that you are using the video. Thank you.",
            "num_replies": 0
        },
        {
            "comment_id": "UgwBpiG7WLW4FPKg7ap4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@fly_8659",
            "date": "2025-01-30",
            "likes": 1,
            "comment": "A great conversation ... ruined by bad audio. I had to mute and use subtitles, the cracks, pops and SSSSSSSSS hurts too much. When you see Quentin, Edward, Kashif or Lewis in person next, please recite a tongue twister about sally and sea shells around 2cm from their ear-hole.  \n\nNo need for a fancy streamer setup. A $12 lavalier lapel mic would fix 90% of the shitty audio. Check your computer or docking station, if it has a single 3.5mm audio TRRS jack (\ud83c\udfa7 / \ud83c\udf99), get a TRRS to 3.5mm Y splitter, if there is no 3.5mm audio jack, grab a 3.5mm microphone to USB-C adapter.",
            "num_replies": 0
        },
        {
            "comment_id": "Ugw7m883iGDO2XI7Swd4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@kwhandy",
            "date": "2025-01-29",
            "likes": 0,
            "comment": "omg idk tech company do research journal too these days",
            "num_replies": 0
        },
        {
            "comment_id": "UgyoM3apZkfkVpip4K94AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@SapienSpace",
            "date": "2025-01-28",
            "likes": 0,
            "comment": "Looking at both the GRPO and GAE, especially GAE, if you dig into the details, this is an adaptive control system (likely using Fuzzy Logic nodes as attention heads that account for KL).",
            "num_replies": 2
        },
        {
            "comment_id": "UgwXUNXuY3DCpOtu1cV4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@IVAN-zg9vl",
            "date": "2025-01-28",
            "likes": 1,
            "comment": "Current AI systems struggle with context-aware reasoning, dynamic constraint satisfaction, and self-corrective adaptability. The Magic Token Framework addresses these gaps through a mathematically rigorous integration of hierarchical context modelling, Bayesian preference learning, and distributed constraint optimization, augmented by a metacognitive layer for real-time self-monitoring and adaptation. Empirical validation across healthcare, autonomous systems, and personalized recommendation tasks demonstrates statistically significant improvements in accuracy (15\u201325%), constraint satisfaction (30\u201340% error reduction), and computational efficiency. The framework is backed by formal proofs of convergence, stability, and fairness, establishing it as a robust, scalable solution for high-stakes decision-making.",
            "num_replies": 2
        },
        {
            "comment_id": "UgzMoMWkFTGtRD2Ufq54AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@willdoit222",
            "date": "2025-01-28",
            "likes": 2,
            "comment": "Real AI genius will never be on YouTube \ud83d\ude02",
            "num_replies": 3
        },
        {
            "comment_id": "UgxsFs1nWW83fdl99594AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@StudyMart",
            "date": "2025-01-27",
            "likes": 0,
            "comment": "Thank you so much",
            "num_replies": 0
        },
        {
            "comment_id": "UgwotjtwnEx__mCGkX94AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@elpablitorodriguezharrera",
            "date": "2025-01-27",
            "likes": 0,
            "comment": "What novel ideas about r1?",
            "num_replies": 2
        },
        {
            "comment_id": "UgymzOYqIQPrqd_ZWkZ4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@TimeStampBuddy",
            "date": "2025-01-27",
            "likes": 20,
            "comment": "00:00:04 - Intro & RL methods\n00:02:47 - Distillation & models\n00:11:25 - Training process\n00:15:52 - Reward & Metrics\n00:21:38 - Distillation & Results\n00:32:00 - What didn't work\n00:42:37 - Model experiences",
            "num_replies": 0
        },
        {
            "comment_id": "Ugy24fiwJAfYifxG1v54AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@DevNerdNobody",
            "date": "2025-01-27",
            "likes": 4,
            "comment": "great video but 720p in 2025 ?",
            "num_replies": 1
        },
        {
            "comment_id": "Ugx0vlm7yjKLHieUbxJ4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@OmNamahShivaaya",
            "date": "2025-01-27",
            "likes": 11,
            "comment": "Its time to learn Mandarin.",
            "num_replies": 1
        },
        {
            "comment_id": "UgxvjTG_NwXZwDFNWed4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@FREELEARNING",
            "date": "2025-01-26",
            "likes": 0,
            "comment": "Thanks for sharing.\nDo you think the performance of the model over non-Latin Languages also is improved in this model.",
            "num_replies": 0
        },
        {
            "comment_id": "Ugxr8NC1vJmCM3HiXBl4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@peterkonrad4364",
            "date": "2025-01-25",
            "likes": 1,
            "comment": "on livebench the qwen 32b distill is just one place above phi4 with an average score of 42. sonnet 3.5 has 59 and is way above them. how does this go together with their statement that a qwen 1.5b distill is better than sonnet 3.5 or gpt 4o?",
            "num_replies": 0
        },
        {
            "comment_id": "UgyIVQ4sgjXEyWQmSyF4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@sonGOKU-gy7rg",
            "date": "2025-01-25",
            "likes": 0,
            "comment": "Does it deal with sound files ?",
            "num_replies": 2
        },
        {
            "comment_id": "UgxDeMrCwL1YhIRyajp4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@ankitsapkota3713",
            "date": "2025-01-25",
            "likes": 3,
            "comment": "We need more of these",
            "num_replies": 0
        },
        {
            "comment_id": "UgzgyN-pqDHWDLCudPd4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@bhargavk1515",
            "date": "2025-01-24",
            "likes": 0,
            "comment": "This is god's work...keep pushing...",
            "num_replies": 0
        },
        {
            "comment_id": "UgzxbVBtgJrjrKGjH8d4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@dariusduesentrieb",
            "date": "2025-01-23",
            "likes": 0,
            "comment": "Very nice",
            "num_replies": 0
        },
        {
            "comment_id": "UgxRWW98wU3EM7ewYsR4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@mahmoudtarek6859",
            "date": "2025-01-23",
            "likes": 7,
            "comment": "Great to observe beautiful minds brainstorm, thanks for sharing \u270c",
            "num_replies": 0
        },
        {
            "comment_id": "UgxKrJWOxWN_pSbG4wh4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@mircorichter1375",
            "date": "2025-01-23",
            "likes": 9,
            "comment": "So they used RL to optimize the LLM on the same benchmarks that they then also use for comparison against say o1 ... I think that is bad science. What is needed is comparison on untrained benchmark sets to evaluate generalization",
            "num_replies": 5
        },
        {
            "comment_id": "Ugxe7fWVy43E4khkxO54AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@aqua200546",
            "date": "2025-01-23",
            "likes": 1,
            "comment": "This is a great video - thanks for putting it up",
            "num_replies": 0
        },
        {
            "comment_id": "UgxMnI7p0QoJ32d0zyB4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@NLPprompter",
            "date": "2025-01-23",
            "likes": 56,
            "comment": "i wish you guys use RVC or something to process mic input... nowdays we don't need expensive mic we can use ML to have clean and clear voice...",
            "num_replies": 5
        },
        {
            "comment_id": "Ugzppi0o-lQFF8DrAXN4AaABAg",
            "video_id": "1xDVbu-WaFo",
            "author": "@nithyaaishu8291",
            "date": "2025-01-23",
            "likes": 12,
            "comment": "Wow !!! We need such discussions. Good to understand a lot of concepts.",
            "num_replies": 0
        }
    ]
}